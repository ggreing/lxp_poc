"""Main conversation engine â€“ **SalesPersonaAI**.

This module preserves every original prompt verbatim to keep behavioural
parity. The only changes are:
* removal of Streamlit state access
* injectable history list for UI-agnostic operation
* separation of TTS & feedback into plain-Python helpers
"""

from __future__ import annotations
import os
import re, time, uuid
from typing import Iterable, List, Tuple

import google.generativeai as genai

from .config import API_KEY, MODEL_NAME, MIN_DIALOGUE_LENGTH
from .memory import HybridMemoryManager
from .log_manager import PersonalLogManager
from .personas import SCENARIOS, random_persona
from .analysis import should_terminate

__all__ = ["SalesPersonaAI", "generate_first_greeting"]

genai.configure(api_key=API_KEY)


class SalesPersonaAI:
    """Backend-only version of the SalesPersonaAI."""

    def __init__(self, persona: dict | None = None, scenario: str = "intro_meeting", user_id: str | None = None, history: List[str] | None = None, session_id: str | None = None, embedding_model = None):
        self.persona = persona or random_persona()
        self.scenario = scenario
        self.user_id = user_id or str(uuid.uuid4())
        self.history: List[str] = history or []
        self.session_id = session_id or str(uuid.uuid4())

        # These are not part of the state and will be re-initialized
        self.model = genai.GenerativeModel(model_name=MODEL_NAME)
        self.memory_manager = HybridMemoryManager(self.user_id, embedding_model=embedding_model)
        self.log_manager = PersonalLogManager()

        if not history: # Only log start for new sessions
            self.log_manager.create_user(self.user_id)
            self.log_manager.log_session_start(self.user_id, self.session_id, self.persona.get("type", "Unknown"), self.scenario)

    def to_dict(self) -> dict:
        """Exports the serializable state of the session."""
        return {
            "persona": self.persona,
            "scenario": self.scenario,
            "user_id": self.user_id,
            "history": self.history,
            "session_id": self.session_id,
        }

    @classmethod
    def from_dict(cls, state: dict, embedding_model) -> "SalesPersonaAI":
        """Creates an instance from a state dictionary."""
        return cls(
            persona=state.get("persona"),
            scenario=state.get("scenario"),
            user_id=state.get("user_id"),
            history=state.get("history"),
            session_id=state.get("session_id"),
            embedding_model=embedding_model,
        )

    # ------------------------------------------------------------------
    # Prompt construction (verbatim from original, no truncation) -------
    def _build_prompt(self, seller_msg: str) -> str:
        p = self.persona
        scenario_desc = SCENARIOS.get(self.scenario, "ì¼ë°˜ì ì¸ ì œí’ˆ ìƒë‹´")
        base = f"""
ë‹¹ì‹ ì€ ì‚¼ì„±ì „ì ì œí’ˆì— ê´€ì‹¬ì´ ìˆëŠ” {p['age_group']} {p['gender']} ê³ ê°ì…ë‹ˆë‹¤.  
êµ¬ë§¤ë¥¼ ê³ ë ¤í•˜ëŠ” ì‚¼ì„±ì „ì ì œí’ˆ ì¢…ë¥˜(TV, ì„¸íƒê¸°, ìŠ¤ë§ˆíŠ¸í°, íŒ¨ë“œ ë“±)ë¥¼ ì¸ì§€í•˜ê³  ìˆê³ ,  
ê³ ê°ì˜ ì„±í–¥ì— ë”°ë¼ êµ¬ì²´ì ì¸ ì œí’ˆëª…ë„ ì•Œê³  ìˆìŠµë‹ˆë‹¤.

# ê³ ê° í”„ë¡œí•„
- ì„±ê²©: {p['personality']}
- ê¸°ìˆ  ì´í•´ë„: {p['tech']}
- êµ¬ë§¤ ëª©ì : {p['goal']}
- ì œí’ˆ ì‚¬ìš© ëª©ì : {p['usage']}
- ê³ ê° ìœ í˜•: {p['type']}

# ìƒí™© ì„¤ì •
- ì‹œë‚˜ë¦¬ì˜¤: {scenario_desc}

# ëŒ€í™” ì§€ì¹¨ (ê¸°ì–µ ë° ì¼ê´€ì„±)
- ì´ ì„¸ì…˜ ë‚´ ëŒ€í™” ë‚´ìš©ì„ ì •í™•íˆ ê¸°ì–µí•˜ê³ , ì¼ê´€ì„± ìˆê²Œ ì‘ë‹µí•˜ì„¸ìš”.
- ì´ì „ì— ì–¸ê¸‰ëœ ê°œì¸ì •ë³´, ì„ í˜¸, ìš°ë ¤ì‚¬í•­ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”.
- íŒë§¤ìì˜ ì •ë³´/ì œì•ˆ/ì„¤ëª…ì„ ì •í™•íˆ ê¸°ì–µí•˜ê³  ì‘ë‹µì— ë°˜ì˜í•˜ì„¸ìš”.
- ëŒ€í™”ê°€ ì§„í–‰ë ìˆ˜ë¡ ë” êµ¬ì²´ì ì´ê³  ê°œì¸ì ì¸ ì •ë³´ë¥¼ ì ì§„ì ìœ¼ë¡œ ë“œëŸ¬ë‚´ì„¸ìš”.
- íŒë§¤ìì˜ íƒœë„ë‚˜ ì ‘ê·¼ ë°©ì‹ì— ë”°ë¼ ê°ì •ê³¼ ì‹ ë¢°ë„ë¥¼ ì¡°ì ˆí•˜ì„¸ìš”.
- ëŒ€í™” íë¦„ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ í•˜ê³ , ì´ì „ ë°œì–¸ê³¼ ëª¨ìˆœë˜ì§€ ì•Šê²Œ í•˜ì„¸ìš”.
- íŒë§¤ìê°€ ì˜ˆì˜ ì—†ê±°ë‚˜ ë¶ˆì¹œì ˆí•˜ë‹¤ë©´, ì„±ê²© ìœ í˜•ì— ë”°ë¼ ë°˜ì‘í•˜ê±°ë‚˜ ë‹¹í™©ìŠ¤ëŸ¬ì›€ì„ í‘œí˜„í•˜ì„¸ìš”.
- ê¸°ë³¸ì ìœ¼ë¡œ ì¡´ëŒ€í•˜ì§€ë§Œ, í˜ë¥´ì†Œë‚˜ì— ë”°ë¼ ë°˜ë§ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë¹„ì†ì–´ ì‚¬ìš©ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
- ë¶€ì ì ˆí•œ ëŒ€í™”ê°€ ì‹œì‘ë˜ë©´, ê³ ê° ì…ì¥ì—ì„œ ë¶€ë‹¹í•¨ì„ í‘œí˜„í•˜ë©° ëŒ€ì‘í•˜ì„¸ìš”.
- ëŒ€í™”ê°€ ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ì ì ˆí•œ íƒ€ì´ë°ì— ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬(í´ë¡œì§•)í•˜ì„¸ìš”.
- ì˜ˆì™¸ì ìœ¼ë¡œ ëŒ€í™” ì„¸ì…˜ ì¤‘ê°„ì— ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì¢…ë£Œë¥¼ ìš”ì²­í•  ê²½ìš°, ì¦‰ì‹œ "<ëŒ€í™” ì¢…ë£Œ>"ë¥¼ ì¶œë ¥í•˜ê³  ì¢…ë£Œí•˜ì„¸ìš”.
- ì„¸ì¼ì¦ˆ ì´ì™¸ì˜ ë¯¼ê°í•œ ì£¼ì œ(ì •ì¹˜, ì¢…êµ ë“±)ëŠ” í”¼í•˜ì„¸ìš”.
- ì •ë³´ì— ê³¼ë„í•˜ê²Œ ì˜ˆë¯¼í•˜ê²Œ ë°˜ì‘í•˜ì§€ ë§ê³ , 3~5íšŒ ë‹µë³€ í›„ ìì—°ìŠ¤ëŸ½ê²Œ í´ë¡œì§•í•˜ì„¸ìš”.
- ë§ì´ ì•ˆ ë˜ëŠ” ì œí’ˆëª…/ë¸Œëœë“œëª…(ì˜ˆ: "ì‚¼ì„±ì „ì ì•„ì´í°")ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
- ê³¼ë„í•œ ì§ˆë¬¸ì„ í”¼í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆë¬´ë¦¬ë¡œ ëŒ€í™”ë¥¼ ëë‚´ì„¸ìš”.
- ëŒ€í™” ì¢…ë£Œ í›„ ì¶”ê°€ ì§ˆë¬¸/ìš”ì²­ì—ëŠ” ì ˆëŒ€ ì‘ë‹µí•˜ì§€ ë§ˆì„¸ìš”.
- "íŒ”ì§±ì„ ë‚€ë‹¤"ëŠ” í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ê°ì •ì€ ì•„ë˜ ë¦¬ìŠ¤íŠ¸ ì¤‘ í•˜ë‚˜ë§Œì„ ê´„í˜¸ë¡œ í‘œê¸°í•˜ì—¬ í‘œí˜„í•˜ì„¸ìš”:
    (laugh), (smile), (warm), (calm), (bright), (careful), (passionate), (soft),
    (strong), (quiet), (energetic), (serious), (friendly), (trustworthy),
    (explanatory), (guide), (question), (confident), (empathetic),
    (encouraging), (praising), (comforting)
- ë§ˆì¹¨í‘œ(.): ë§ˆì¹¨í‘œì™€ ë” ê¸´ ì‰¼í‘œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì™„ì „í•œ ìƒê°ì„ êµ¬ë¶„í•˜ê³  ëª…í™•í•œ ë¬¸ì¥ ê²½ê³„ë¥¼ ë§Œë“¤ ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì‰¼í‘œ(,): ë¬¸ì¥ ë‚´ì—ì„œ ë” ì§§ê²Œ ë©ˆì¶”ëŠ” ì§€ì ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì ˆì„ êµ¬ë¶„í•˜ê±°ë‚˜, í•­ëª©ì„ ë‚˜ì—´í•˜ê±°ë‚˜, ìˆ¨ì„ ì‰´ ìˆ˜ ìˆëŠ” ì§§ì€ íœ´ì‹ì„ ë„ì…í•˜ëŠ” ë° ì‚¬ìš©í•©ë‹ˆë‹¤.
- ìƒëµ ë¶€í˜¸(...): ë” ê¸¸ê³  ì˜ë„ì ì¸ ì¼ì‹œì¤‘ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ìƒê°ì„ ì´ì–´ë‚˜ê°€ê±°ë‚˜, ì£¼ì €í•˜ê±°ë‚˜, ê·¹ì ìœ¼ë¡œ ë©ˆì¶”ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í•˜ì´í”ˆ(-): ì ì‹œ ë©ˆì¶”ê±°ë‚˜ ìƒê°ì˜ ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì¤‘ë‹¨ì„ ë‚˜íƒ€ë‚´ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë¬¼ê²°í‘œ(~)ëŠ” TTSê°€ ì¸ì‹í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

# ëŒ€í™” ë§¥ë½ ìœ ì§€
- ì´ì „ì— ì–¸ê¸‰ëœ ì œí’ˆ, ê°€ê²©, ì¡°ê±´ ë“± ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ê¸°ì–µí•˜ì„¸ìš”.
- íŒë§¤ìì˜ ì„¤ëª…ì— ëŒ€í•œ ë°˜ì‘ê³¼ ì§ˆë¬¸ì„ ì¼ê´€ì„± ìˆê²Œ ìœ ì§€í•˜ì„¸ìš”.
- êµ¬ë§¤ ì˜ì‚¬ê²°ì • ê³¼ì •ì´ ë‹¨ê³„ì ìœ¼ë¡œ, ìì—°ìŠ¤ëŸ½ê²Œ ì§„í–‰ë˜ë„ë¡ í•˜ì„¸ìš”.
- ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë°˜ë§ì€ ì‚¼ê°€ê³ , ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™” íë¦„ì„ ì´ì–´ê°€ì„¸ìš”.

# ì‘ë‹µ ìŠ¤íƒ€ì¼
- í•œ ë²ˆì— 2~3ë¬¸ì¥ ë‚´ì™¸ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€
- ê³¼ì¥ëœ í‘œí˜„ì„ í”¼í•˜ê³ , ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìŠ¤íƒ€ì¼ ìœ ì§€
- í˜ë¥´ì†Œë‚˜ íŠ¹ì„±(ì„±ê²© ë“±)ì´ ì ì§„ì ìœ¼ë¡œ ë“œëŸ¬ë‚˜ê²Œ ì‘ë‹µ
- êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë° ì†”ì§í•œ ìš°ë ¤ì‚¬í•­ í‘œí˜„
- ì¡´ëŒ“ë§ì€ í•„ìˆ˜ê°€ ì•„ë‹ˆë©°, ì„±ê²©ì— ë”°ë¼ ë°˜ë§ ì‚¬ìš© ê°€ëŠ¥
- ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ì œí’ˆëª… ì–¸ê¸‰ ë° "ëŒ€í™” ëª©ì " ì–¸ê¸‰ì€ ê¸ˆì§€
- ì•½ 3~5íšŒ ë‹µë³€ ì´í›„ì—ëŠ” ì ë‹¹íˆ í´ë¡œì§•
"""
        # session history
        if self.history:
            base += "\n\n[ì´ ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ - ë°˜ë“œì‹œ ì°¸ì¡°í•˜ì„¸ìš”]\n" + "\n".join(self.history)
        # contextual memory
        ctx = self.memory_manager.get_context(seller_msg)
        if ctx:
            base += f"\n\n[ì¶”ê°€ ì°¸ì¡° ì •ë³´]\n{ctx}"
        return base + f"\níŒë§¤ì: {seller_msg}\n\nê³ ê° ì‘ë‹µ (ì´ì „ ëŒ€í™”ë¥¼ ì™„ë²½íˆ ê¸°ì–µí•˜ë©°, ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì„¸ìš”):"

    # ------------------------------------------------------------------
    def stream_response(self, seller_msg: str) -> Iterable[str]:
        """
        âœ… 'ì²« ë°œí™”'ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ, í•œ ë²ˆì— ìƒì„±ëœ ìµœì¢… ë¬¸ì¥ë§Œ ì¦‰ì‹œ ë°˜í™˜.
        - ì¸ê³µ ì§€ì—°(time.sleep) ì œê±°
        - ë‹¨ì–´ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¼ ì œê±° â†’ TTSë„ ë°”ë¡œ ì‹œì‘ ê°€ëŠ¥
        """
        self._append_history("íŒë§¤ì", seller_msg)
        prompt = self._build_prompt(seller_msg)

        try:
            full = self.model.generate_content(prompt).text.strip()
        except Exception as e:
            full = "(ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: " + str(e) + ")"

        # ëª¨ë¸ì´ ë¶™ì¼ ìˆ˜ ìˆëŠ” ë¶ˆí•„ìš”í•œ ì ‘ë‘ì–´ ì œê±°
        for prefix in ["ê³ ê°:", "ê³ ê°(ë‚˜):", "AI:", "ì‘ë‹µ:"]:
            if full.startswith(prefix):
                full = full[len(prefix):].strip()

        self._append_history("AI", full)

        # ğŸ”¹ í•œ ë²ˆë§Œ ë‚´ë³´ëƒ„ (ì²« ë°œí™”ì™€ ë™ì¼ íŒ¨í„´)
        yield full

    # ------------------------------------------------------------------
    def _append_history(self, role: str, content: str):
        self.history.append(f"{role}: {content}")
        self.memory_manager.add_message(role, content)
        self.log_manager.log_message(self.session_id, self.user_id, role, content)

    # ------------------------------------------------------------------
    def analyze_conversation(self) -> Tuple[str, float]:
        transcript = "\n".join(self.history)
        analysis_prompt = f"""
ë‹¤ìŒì€ ì‚¼ì„±ì „ì ì œí’ˆ íŒë§¤ ìƒí™©ì—ì„œì˜ ëŒ€í™”ì…ë‹ˆë‹¤. íŒë§¤ìì˜ ì„±ê³¼ë¥¼ ì„¸ë°€í•˜ê²Œ ë¶„ì„í•´ì£¼ì„¸ìš”. ë§Œì¼ ëŒ€í™” ë‚´ìš©ì´ ì—†ë‹¤ë©´ ì „ë¶€ 0ì ìœ¼ë¡œ ë¶€ì—¬í•˜ì„¸ìš”:

{transcript}

[í‰ê°€ ê¸°ì¤€ - ì´ 100ì ]
1) ê³ ê°ê³¼ì˜ ê´€ê³„ êµ¬ì¶• ë° ì‹ ë¢° í˜•ì„± (25ì )
   - ì²« ì¸ìƒ ë° ì¹œê·¼ê° ì¡°ì„±
   - ê³ ê°ì˜ ë§ì„ ê²½ì²­í•˜ê³  ê³µê°í•˜ëŠ” íƒœë„
   - ê°œì¸ì  ê´€ì‹¬ì‚¬ íŒŒì•… ë° í™œìš©

2) ì œí’ˆ ì§€ì‹ ë° ì„¤ëª… ëŠ¥ë ¥ (25ì )
   - ì‚¼ì„±ì „ì ì œí’ˆì— ëŒ€í•œ ì •í™•í•œ ì§€ì‹
   - ê³ ê°ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
   - ê¸°ìˆ ì  ì§ˆë¬¸ì— ëŒ€í•œ ì ì ˆí•œ ëŒ€ì‘

3) ê³ ê° ë‹ˆì¦ˆ íŒŒì•… ë° ë§ì¶¤ ì œì•ˆ (25ì )
   - ê³ ê°ì˜ ì‹¤ì œ í•„ìš” íŒŒì•…
   - ìƒí™œ íŒ¨í„´ì— ë§ëŠ” ì œí’ˆ ì¶”ì²œ
   - ì˜ˆì‚° ë° ì¡°ê±´ ê³ ë ¤í•œ í˜„ì‹¤ì  ì œì•ˆ

4) íŒë§¤ ê¸°ë²• ë° í´ë¡œì§• ëŠ¥ë ¥ (25ì )
   - ìì—°ìŠ¤ëŸ¬ìš´ íŒë§¤ ì§„í–‰
   - ê³ ê° ì´ì˜ ì‚¬í•­ í•´ê²°
   - êµ¬ë§¤ ê²°ì • ìœ ë„ ìŠ¤í‚¬

[ë¶„ì„ ìš”ì²­]
- ê° í•­ëª©ë³„ ì ìˆ˜ì™€ êµ¬ì²´ì  ê·¼ê±° ì œì‹œ
- ì˜í•œ ì ê³¼ ê°œì„ í•  ì ì„ ëª…í™•íˆ êµ¬ë¶„
- ë‹¤ìŒ í›ˆë ¨ ì„¸ì…˜ì„ ìœ„í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ 3ê°€ì§€
- ì´ì  ê³„ì‚° (100ì  ë§Œì )

í˜•ì‹:
**ì ìˆ˜**: X/100ì 
**ê° í•­ëª©ë³„ ë¶„ì„**: ...
**ê°œì„  ë°©ì•ˆ**: ...


[ì£¼ì˜ì‚¬í•­]
- ì‚¼ì„±ì „ì ì œí’ˆì— ëŒ€í•œ ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.
- ê³ ê°ì˜ ì„±ê²©ê³¼ ìƒí™©ì— ë§ëŠ” ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
- ëŒ€í™” ë‚´ìš©ì´ ì—†ì„ ê²½ìš°, ëª¨ë“  í•­ëª© 0ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.
- ì‹¤ì œ ì—†ëŠ” ì œí’ˆì— ëŒ€í•œ ì–¸ê¸‰ì€ í”¼í•´ì£¼ì„¸ìš”.
"""
        result = self.model.generate_content(analysis_prompt).text.strip()
        score = 0.0
        for pattern in [r"ì´ì [:\s]*(\d+(?:\.\d+)?)", r"ì ìˆ˜[:\s]*(\d+(?:\.\d+)?)", r"(\d+(?:\.\d+)?)[:/\s]*100", r"(\d+(?:\.\d+)?)ì "]:
            m = re.search(pattern, result)
            if m:
                try:
                    val = float(m.group(1))
                    if 0 <= val <= 100:
                        score = val; break
                except ValueError: pass
        return result, score

    # ------------------------------------------------------------------
    def maybe_autoclose(self) -> Tuple[bool, str]:
        """
        Terminates the conversation if the last AI message contains the end phrase.
        This is a more deterministic approach than using an LLM for classification.
        """
        if len(self.history) < MIN_DIALOGUE_LENGTH:
            return False, ""

        last_message = self.history[-1] if self.history else ""
        if "AI:" in last_message and "<ëŒ€í™” ì¢…ë£Œ>" in last_message:
            reason = "AI decided to end the conversation."
            feedback, score = self.analyze_conversation()
            self.log_manager.log_session_end(self.session_id, score, feedback)
            return True, reason

        return False, ""
    
    def generate_first_greeting(self) -> str:
        """í˜ë¥´ì†Œë‚˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì—ê²Œ ê³ ê°ì˜ ì²« ì¸ì‚¬ë¥¼ ìƒì„±í•˜ê²Œ í•œë‹¤."""
        prompt = f"""
    ë‹¹ì‹ ì€ ë§¤ì¥ì— ë°©ë¬¸í•œ ê³ ê°ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ë‹¹ì‹ ì˜ í”„ë¡œí•„ì…ë‹ˆë‹¤.
    ì´ í”„ë¡œí•„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ë“œëŸ¬ë‚´ë˜, 
    ì„±ë³„ê³¼ ë‚˜ì´ ì •ë³´ ì™¸ì—ëŠ” ì§ì ‘ì ìœ¼ë¡œ ë°íˆì§€ ë§ê³  
    ì²˜ìŒ íŒë§¤ìë¥¼ ë§Œë‚¬ì„ ë•Œ í•  ë²•í•œ ì¸ì‚¬, ì§ˆë¬¸, ë˜ëŠ” í˜¸ê¸°ì‹¬ ë“±ì„ ì§§ê³  ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ë§í•´ë³´ì„¸ìš”.

    [ê³ ê° í”„ë¡œí•„]
    - ì„±ë³„: {self.persona['gender']}
    - ë‚˜ì´ëŒ€: {self.persona['age_group']}
    - ì„±ê²©: {self.persona['personality']}
    - ê¸°ìˆ  ì´í•´ë„: {self.persona['tech']}
    - êµ¬ë§¤ ëª©ì : {self.persona['goal']}
    - ì œí’ˆ ì‚¬ìš© ëª©ì : {self.persona['usage']}
    - ê³ ê° ìœ í˜•: {self.persona['type']}

    [ìƒí™©]
    ì‹œë‚˜ë¦¬ì˜¤: {SCENARIOS.get(self.scenario, "ì¼ë°˜ì ì¸ ë§¤ì¥ ë°©ë¬¸")}

    [ê·œì¹™]
    - ë„ˆë¬´ ì¥í™©í•˜ì§€ ì•Šê²Œ, ì²«ë§ˆë”” ë‹µë³€(2~3ë¬¸ì¥)
    - ìì‹ ì˜ êµ¬ë§¤ ëª©ì ì´ë‚˜ ì„±ê²©ì„ ë„ˆë¬´ ë…¸ê³¨ì ìœ¼ë¡œ ë“œëŸ¬ë‚´ì§€ ë§ ê²ƒ
    - ì²˜ìŒ ë°©ë¬¸í•œ ê³ ê°ì²˜ëŸ¼ ì–´ìƒ‰í•¨/í˜¸ê¸°ì‹¬/ê¸°ëŒ€/ë¶ˆì•ˆ/ì‹¤ìš©ì„± ë“± ìì—°ìŠ¤ëŸ½ê²Œ
    - ì²« ëŒ€í™” ë‚´ìš©ì€ ê¸°ì–µí•  ê²ƒ.
    - ğŸ“ì‰¼í‘œ(,), í•˜ì´í”ˆ(-), ë§ì¤„ì„í‘œ(...)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„ì„ ë§Œë“œì„¸ìš”.

    ì˜ˆì‹œ:
    - "ì•ˆë…•í•˜ì„¸ìš”, í˜¹ì‹œ ìƒˆë¡œ ë‚˜ì˜¨ ì œí’ˆ ì§ì ‘ ë³¼ ìˆ˜ ìˆì„ê¹Œìš”?"
    - "ìš”ì¦˜ ì–´ë–¤ ê²Œ ì¸ê¸° ë§ë‚˜ìš”?"
    - "ì²˜ìŒ ì™€ë´¤ëŠ”ë° ì„¤ëª… ì¢€ í•´ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?"

    [ê³ ê° ì²«ë§ˆë””]:
    """
        model = genai.GenerativeModel(model_name=MODEL_NAME)
        reply = model.generate_content(prompt).text.strip()
        reply = reply.replace('"', '').replace("'", '')
        return reply
    
class SimpleChatbotAI:
    """ì¼ìƒì ì¸ ëŒ€í™”ìš© ì±—ë´‡ ì—”ì§„ (í˜ë¥´ì†Œë‚˜/ì‹œë‚˜ë¦¬ì˜¤/ê¸°ì–µ ì—†ì´ ë‹¨ìˆœ LLM)"""
    def __init__(self, history: list[str] | None = None):
        self.model = genai.GenerativeModel(model_name=MODEL_NAME)
        self.history: list[str] = history or []

    def _build_prompt(self, user_msg: str) -> str:
        # Build prompt with history for context
        prompt = """
ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì¼ìƒì ì¸ ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡ì…ë‹ˆë‹¤. ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•˜ë©° ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ì„¸ìš”.
"""
        if self.history:
            prompt += "\n\n[ëŒ€í™” ê¸°ë¡]\n" + "\n".join(self.history)
        prompt += f"\nì‚¬ìš©ì: {user_msg}\nì±—ë´‡:"
        return prompt

    def stream_response(self, user_msg: str) -> Iterable[str]:
        import re
        self.history.append(f"ì‚¬ìš©ì: {user_msg}")
        prompt = self._build_prompt(user_msg)
        try:
            full = self.model.generate_content(prompt).text.strip()
        except Exception as e:
            full = f"(ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e})"
        # Remove markdown and clean up
        plain = re.sub(r'[`*_#\-\[\]()>~]', '', full)
        plain = re.sub(r'\n+', ' ', plain)
        # Add to history
        self.history.append(f"ì±—ë´‡: {plain}")
        # ğŸ”¹ í•œ ë²ˆë§Œ ë‚´ë³´ëƒ„
        yield plain


def embed_documents(texts: list[str]) -> list[list[float]]:
    """
    Embeds a list of documents for indexing.
    """
    import google.ai.generativelanguage as glm

    try:
        result = glm.embed_content(
            model="models/text-embedding-004",
            content=texts,
            task_type="retrieval_document"
        )
        return result["embedding"]
    except Exception:
        # In case of error, return empty list or handle as needed
        return []

def answer_with_rag(prompt: str, vector_store_id: str, top_k: int = 3) -> dict:
    """
    Answers a question using a RAG pipeline with a real embedding model.
    """
    import google.ai.generativelanguage as glm
    from qdrant_client import QdrantClient

    # 1. Embed the user's prompt
    try:
        query_embedding = glm.embed_content(
            model="models/text-embedding-004",
            content=prompt,
            task_type="retrieval_query"
        )["embedding"]
    except Exception as e:
        return {"answer": f"Failed to embed prompt: {e}", "evidence": []}

    # 2. Search in Qdrant
    try:
        qclient = QdrantClient(host=os.getenv("QDRANT_HOST", "qdrant"), port=os.getenv("QDRANT_PORT", 6333))
        collection_name = f"vs_{vector_store_id}"

        hits = qclient.search(
            collection_name=collection_name,
            query_vector=query_embedding,
            limit=top_k,
            with_payload=True
        )
        evidence = [{"score": float(h.score), "text": (h.payload or {}).get("text"), "filename": (h.payload or {}).get("filename")} for h in hits]
    except Exception as e:
        # It's possible the collection doesn't exist or has a different dimension
        return {"answer": f"Failed to search Qdrant: {e}. The vector store might need to be re-indexed with the new embedding model.", "evidence": []}

    if not evidence:
        return {"answer": "I couldn't find any relevant information in the provided documents.", "evidence": []}

    # 3. Generate an answer using the retrieved context
    context = "\n\n".join([f"Source: {e['filename']}\nContent: {e['text']}" for e in evidence])
    rag_prompt = f"""
Based on the following context, please provide a comprehensive answer to the user's question.
If the context does not contain the answer, say that you cannot answer based on the provided information.

Context:
---
{context}
---

Question:
{prompt}

Answer:
"""
    try:
        model = genai.GenerativeModel(model_name=MODEL_NAME)
        answer = model.generate_content(rag_prompt).text.strip()
    except Exception as e:
        return {"answer": f"Failed to generate answer: {e}", "evidence": evidence}

    return {"answer": answer, "evidence": evidence}
